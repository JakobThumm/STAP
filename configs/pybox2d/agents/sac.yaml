agent: SAC
agent_kwargs:
  tau: 0.005
  init_temperature: 0.1
  critic_freq: 1
  actor_freq: 1
  target_freq: 2
  init_steps: 5000

  network_class: ActorCriticPolicy
  network_kwargs:
    actor_class: DiagonalGaussianMLPActor
    actor_kwargs:
      hidden_layers: [256, 256]
      log_std_bounds: [-5, 2]
    critic_class: ContinuousMLPCritic
    critic_kwargs:
      hidden_layers: [256, 256]
      num_q_fns: 2
    ortho_init: true

  dataset_class: ReplayBuffer
  dataset_kwargs:
    capacity: 1000000
    batch_size: 1024
    sample_strategy: uniform
    save_frequency: 1000

  optimizer_class: Adam
  optimizer_kwargs:
    lr: 0.0001

train_kwargs: # Arguments given to Algorithm.train
  total_steps: 250000 # The total number of steps to train
  log_freq: 250 # How often to log values
  eval_freq: 10000 # How often to run evals
  eval_ep: 10 # Number of enviornment episodes to run for evaluation, or -1 if none should be run.
  loss_metric: reward # The validation metric that determines when to save the "best_checkpoint"
  workers: 0 # Number of dataloader workers.
  profile_freq: 50
