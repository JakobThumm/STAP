import functools
from typing import Optional, Sequence, Tuple

import numpy as np
import torch

from temporal_policies import agents, dynamics, envs
from temporal_policies.planners import base as planners
from temporal_policies.planners import utils
from temporal_policies.utils import spaces


class CEMPlanner(planners.Planner):
    """Planner using the Improved Cross Entropy Method."""

    def __init__(
        self,
        policies: Sequence[agents.Agent],
        dynamics: dynamics.Dynamics,
        num_iterations: int = 8,
        num_samples: int = 128,
        num_elites: int = 16,
        standard_deviation: float = 1.0,
        keep_elites_fraction: float = 0.0,
        population_decay: float = 1.0,
        momentum: float = 0.0,
        device: str = "auto",
    ):
        """Constructs the iCEM planner.

        Args:
            policies: Policies used to evaluate trajecotries.
            dynamics: Dynamics model.
            num_iterations: Number of CEM iterations.
            num_samples: Number of samples to generate per CEM iteration.
            num_elites: Number of elites to select from population.
            standard_deviation: Used to sample random actions for the initial
                population. Will be scaled by the action space.
            keep_elites_fraction: Fraction of elites to keep between iterations.
            population_decay: Population decay applied after each iteration.
            momentum: Momentum of distribution updates.
            device: Torch device.
        """
        super().__init__(policies=policies, dynamics=dynamics, device=device)
        self._num_iterations = num_iterations
        self._num_samples = num_samples
        self._num_elites = max(2, min(num_elites, self.num_samples // 2))
        self._standard_deviation = standard_deviation

        # Improved CEM parameters.
        self._num_elites_to_keep = int(keep_elites_fraction * self.num_elites + 0.5)
        self._population_decay = population_decay
        self._momentum = momentum

    @property
    def num_iterations(self) -> int:
        """Number of CEM iterations."""
        return self._num_iterations

    @property
    def num_samples(self) -> int:
        """Number of samples to generate per CEM iteration."""
        return self._num_samples

    @property
    def num_elites(self) -> int:
        """Number of elites to select from population."""
        return self._num_elites

    @property
    def standard_deviation(self) -> float:
        """Unnormalized standard deviation for sampling random actions."""
        return self._standard_deviation

    @property
    def num_elites_to_keep(self) -> int:
        """Number of elites to keep between iterations."""
        return self._num_elites_to_keep

    @property
    def population_decay(self) -> float:
        """Population decay applied after each iteration."""
        return self._population_decay

    @property
    def momentum(self) -> float:
        """Momentum of distribution updates."""
        return self._momentum

    def _compute_initial_distribution(
        self, observation: torch.Tensor, action_skeleton: Sequence[envs.Primitive]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Computes the initial popoulation distribution.

        The mean is generated by randomly rolling out a random trajectory using
        the dynamics model. The standard deviation is scaled according to the
        action space for each action in the skeleton.

        Args:
            observation: Start observation.
            action_skeleton: List of primitives.

        Returns:
            2-tuple (mean, std).
        """
        T = len(action_skeleton)

        # Roll out a trajectory.
        _, actions, _ = self.dynamics.rollout(
            observation, action_skeleton, self.policies
        )
        mean = actions.cpu().numpy()

        # Scale the standard deviations by the action spaces.
        std = spaces.null_tensor(self.dynamics.action_space, (T,)).numpy()
        for t, primitive in enumerate(action_skeleton):
            a = self.policies[primitive.idx_policy].action_space
            std[t] = self.standard_deviation * 0.5 * (a.high - a.low)

        return mean, std

    def plan(
        self, observation: np.ndarray, action_skeleton: Sequence[envs.Primitive]
    ) -> planners.PlanningResult:
        """Runs `num_iterations` of CEM.

        Args:
            observation: Environment observation.
            action_skeleton: List of primitives.

        Returns:
            Planning result.
        """
        num_samples = self.num_samples

        best_actions: Optional[np.ndarray] = None
        best_states: Optional[np.ndarray] = None
        p_best_success: float = -float("inf")
        best_values: Optional[np.ndarray] = None
        visited_actions = []
        visited_states = []
        p_visited_success = []
        visited_values = []

        value_fns = [
            self.policies[primitive.idx_policy].critic for primitive in action_skeleton
        ]
        decode_fns = [
            functools.partial(self.dynamics.decode, primitive=primitive)
            for primitive in action_skeleton
        ]

        # Get initial state.
        with torch.no_grad():
            t_observation = torch.from_numpy(observation).to(self.dynamics.device)

            # Initialize distribution.
            mean, std = self._compute_initial_distribution(
                t_observation, action_skeleton
            )
            elites = np.empty((0, *mean.shape), dtype=mean.dtype)
            p_elites = np.empty(0, dtype=np.float32)

            for _ in range(self.num_iterations):
                # Sample from distribution.
                samples = mean + std * np.random.randn(num_samples, *mean.shape).astype(
                    np.float32
                )
                samples[0] = mean
                for t, primitive in enumerate(action_skeleton):
                    action_space = self.policies[primitive.idx_policy].action_space
                    action_shape = action_space.shape[0]
                    samples[:, t, :action_shape] = np.clip(
                        samples[:, t, :action_shape],
                        action_space.low,
                        action_space.high,
                    )

                # Roll out trajectories.
                policies = [
                    agents.ConstantAgent(
                        action=samples[
                            :,
                            t,
                            : self.policies[primitive.idx_policy].action_space.shape[0],
                        ],
                        policy=self.policies[primitive.idx_policy],
                    )
                    for t, primitive in enumerate(action_skeleton)
                ]
                t_states, t_actions, p_transitions = self.dynamics.rollout(
                    t_observation,
                    action_skeleton,
                    policies,
                    batch_size=num_samples,
                    time_index=True,
                )

                # Evaluate trajectories.
                p_success, t_values = utils.evaluate_trajectory(
                    value_fns, decode_fns, t_states, t_actions, p_transitions
                )

                # Convert to numpy.
                actions = t_actions.cpu().numpy()
                states = t_states.cpu().numpy()
                p_success = p_success.cpu().numpy()
                values = t_values.cpu().numpy()
                visited_actions.append(actions)
                visited_states.append(states)
                p_visited_success.append(p_success)
                visited_values.append(values)

                # Append subset of elites from previous iteration.
                samples = np.concatenate(
                    (samples, elites[: self.num_elites_to_keep]), axis=0
                )
                p_success = np.concatenate(
                    (p_success, p_elites[: self.num_elites_to_keep]), axis=0
                )

                # Sort trajectories in descending order of success probability.
                idx_sorted = np.argsort(p_success)[::-1]
                samples = samples[idx_sorted]
                p_success = p_success[idx_sorted]

                # Compute elites.
                elites = samples[: self.num_elites]
                p_elites = p_success[: self.num_elites]

                # Track best action.
                if p_success[0] > p_best_success:
                    p_best_success = p_success[0]
                    best_actions = actions[idx_sorted[0]]
                    best_states = states[idx_sorted[0]]
                    best_values = values[idx_sorted[0]]

                # Update distribution.
                mean = self.momentum * mean + (1 - self.momentum) * elites.mean(axis=0)
                std = self.momentum * std + (1 - self.momentum) * elites.std(axis=0)

                # Decay population size.
                num_samples = int(self.population_decay * num_samples + 0.5)
                num_samples = max(num_samples, 2 * self.num_elites)

        assert (
            best_actions is not None
            and best_states is not None
            and best_values is not None
        )

        return planners.PlanningResult(
            actions=best_actions,
            states=best_states,
            p_success=p_best_success,
            values=best_values,
            visited_actions=np.concatenate(visited_actions, axis=0),
            visited_states=np.concatenate(visited_states, axis=0),
            p_visited_success=np.concatenate(p_visited_success, axis=0),
            visited_values=np.concatenate(visited_values, axis=0),
        )
